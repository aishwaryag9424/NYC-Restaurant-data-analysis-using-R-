#Install libraries and packages
install.packages("tidyr")
install.packages('hash')
install.packages('dplyr')
install.packages('psych')

#Importing Libraries
library(hash)
library(tidyr)
library(dplyr)
library(psych)

#Code start

set.seed(0)
#getting working directory and the go to session->set working directory- >choose directory getwd()

#dataset1
#importing a csv file for dataset1
data1 <- read.csv(file="food_order.csv", header=TRUE,sep=",") #removing duplicates from data1
data1 <- data1[!duplicated(data1), ]

#Removing NA before plotting boxplot
data1 <- data1 %>% drop_na()

#Box plot after removing outliers
boxplot(data1$cost_of_the_order)

#Removing the outliers
Q1 <- quantile(data1$cost_of_the_order, .25)
Q3 <- quantile(data1$cost_of_the_order, .75)
IQR <- IQR(data1$cost_of_the_order)
new_data2 <- subset(data1, data1$cost_of_the_order> (Q1 - 1.5*IQR) & data1$cost_of_the_order< (Q3 + 1.5*IQR))
new_data2

#There are no outliers in this dataset

#taking subset of data1
data1_subset <- subset(data1, rating!="Not given"& cuisine_type %in%c('Indian','American','Chinese') )
data1_subset

#accessing required data from data1
new_data1 <- data_subset[c("restaurant_name","cuisine_type","cost_of_the_order","rating","d ay_of_the_week","customer_id")]
#assigning sequence to rownames after removing the unnecessary rows rownames new_data1 <- 1:nrow(new_data1)

#removing null values if any
new_data1 <- drop_na(new_data1)

View(new_data1)

#to export the files into csv format
write.csv(new_data1,file="data1.csv")

#Analysis of dataset1

#Reading the csv
data1 <- read.csv(file="data1.csv", header=TRUE,sep=",")
#Getting over all size of data1 num_of_rows_data1 <- nrow(data1)
#length
l1 <- aggregate(x= data1$cost_of_the_order,by = list(data1$day_of_the_week), FUN = length)
l1
l2 <- aggregate(x= data1$cost_of_the_order,by = list(data1$cuisine_type), FUN = length)
l2
l3 <- aggregate(x= data1$cost_of_the_order,by = list(data1$rating), FUN = length)
l3

#summary
full_sum_data1 <- summary(data1)
full_sum_data1
sum_data1
sum_data1
sum_data2
sum_data2
sum_data3
sum_data3
<- tapply(data1$cost_of_the_order, data1$day_of_the_week, summary) <- tapply(data1$cost_of_the_order, data1$cuisine_type, summary)
<- tapply(data1$cost_of_the_order, data1$rating, summary)

#sd
sddata1 <- sd(data1$cost_of_the_order)
gsd1 <- aggregate(x= data1$cost_of_the_order,by = list(data1$day_of_the_week), FUN = sd)
gsd1
gsd2 <- aggregate(x= data1$cost_of_the_order,by = list(data1$cuisine_type), FUN = sd)
gsd2
gsd3 <- aggregate(x= data1$cost_of_the_order,by = list(data1$rating), FUN = sd)
gsd3

#Variance
var_overall = var(data1$cost_of_the_order)
v1 <- aggregate(x= data1$cost_of_the_order,by = list(data1$day_of_the_week), FUN = var)
v1
v2 <- aggregate(x= data1$cost_of_the_order,by = list(data1$rating), FUN = var) v2
v3 <- aggregate(x= data1$cost_of_the_order,by = list(data1$cuisine_type), FUN = var)
v3

#Exponential mean
expmean <- exp(mean(log(data1$cost_of_the_order))) expmean
#exponential mean for day_of_the_week by creating a function exp_day_of_the_week <- function() {
 uniqval <- unique(data1$day_of_the_week)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$day_of_the_week==i] a <- exp(mean(log(data_day_week)))
h[i]<-a
}
return(h) }
exp_day_of_the_week()
exp_rating<- function() {
 uniqval <- unique(data1$rating)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$rating==i] a <- exp(mean(log(data_day_week)))
h[i]<-a
}
return(h) }
exp_rating()
exp_cuisine_type<- function(){
 uniqval <- unique(data1$cuisine_type)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$cuisine_type==i] a <- exp(mean(log(data_day_week)))
h[i]<-a
}
return(h) }
exp_cuisine_type()

#geometric mean
library(psych)
geomean <- geometric.mean(data1$cost_of_the_order) geomean
geo_day_of_the_week <- function() {
 uniqval <- unique(data1$day_of_the_week)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$day_of_the_week==i] a <- geometric.mean(data_day_week)
h[i]<-a
} return(h)
}
geo_day_of_the_week()
geo_rating<- function() {
 uniqval <- unique(data1$rating)
 h <- hash()
for (i in uniqval){
data_rating <- data1$cost_of_the_order[data1$rating==i] a <- geometric.mean(data_rating)
h[i]<-a
}
return(h)
}
geo_rating()
geo_cuisine_type<- function() {
 uniqval <- unique(data1$cuisine_type)
 h <- hash()
for (i in uniqval){
data_cuisine <- data1$cost_of_the_order[data1$ cuisine_type==i] a <- geometric.mean(data_cuisine)
h[i]<-a
}
return(h)
}
geo_cuisine_type()

#Mode
getmode <- function(v) {
 uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))] }
mo <- getmode(data1$cost_of_the_order)
mod_day_of_the_week <- function() {
 uniqval <- unique(data1$day_of_the_week)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$day_of_the_week==i] uniqv <- unique(data_day_week)
a <- uniqv[which.max(tabulate(match(data_day_week, uniqv)))] h[i]<-a
}
return(h) }
mod_day_of_the_week()
mod_day_of_the_week <- function() {
 uniqval <- unique(data1$day_of_the_week)
 h <- hash()
for (i in uniqval){
data_day_week <- data1$cost_of_the_order[data1$day_of_the_week==i] uniqv <- unique(data_day_week)
return(h) }
a <- uniqv[which.max(tabulate(match(data_day_week, uniqv)))] h[i]<-a
}
mod_day_of_the_week()
mod_cuisine_type <- function() {
 uniqval <- unique(data1$cuisine_type)
 h <- hash()
for (i in uniqval){
data_cuisine_type <- data1$cost_of_the_order[data1$cuisine_type==i] uniqv <- unique(data_cuisine_type)
a <- uniqv[which.max(tabulate(match(data_cuisine_type, uniqv)))] h[i]<-a
}
return(h) }
mod_cuisine_type()
#Other Descriptive Statistics describe(data1$cost_of_the_order) describeBy(data1$cost_of_the_order,data1$day_of_the_week) describeBy(data1$cost_of_the_order,data1$rating) describeBy(data1$cost_of_the_order,data1$cuisine_type)

#Plot the chart
boxplot(data1$cost_of_the_order)
#histogram dataset1
hist(data1$cost_of_the_order,xlab = "Cost of Order",col = "lightblue",border = "black", xlim = c(0,40), breaks = 5,main='Histogram of Cost of Order')

##########importing a csv file for dataset2
data2 <- read.csv(file="weather_events_ds2.csv", header=TRUE, sep=",") #remove duplicates from data2
#data2 <- data2[!duplicated(data2), ]

#Changing the format of Start Time Column
data2[['StartTime.UTC.']] <- strptime(data2[['StartTime.UTC.']],format = "%Y- %m-%d %H:%M:%S")
data2[['StartTime.Date']] <- format(data2[['StartTime.UTC.']],"%Y-%m-%d") data2[['StartTime.Time']] <- format(data2[['StartTime.UTC.']],"%H:%M:%S")

#Sorting dataset 2 based on the datetime
data2 <- data2 %>% arrange(data2$StartTime.UTC.)
View(data2)

#Getting the Inter arrival Time
data2 <- data2 %>% mutate(time_diff_col = strptime(data2$StartTime.UTC., "%Y- %m-%d %H:%M:%S") - lag(strptime(data2$StartTime.UTC., "%Y-%m-%d %H:%M:%S"))) View(data2)

#converting Inter arrival time to numeric to do the data analysis data2$time_diff_col <- as.numeric(data2$time_diff_col)
View(data2)

#Removing NA before plotting boxplot
data2 <- data2 %>% drop_na()

#Box plot after removing outliers
boxplot(data2$time_diff_col)

#Removing the outliers
Q1 <- quantile(data2$time_diff_col, .25)
Q3 <- quantile(data2$time_diff_col, .75)
IQR <- IQR(data2$time_diff_col)
new_data2 <- subset(data2, data2$time_diff_col> (Q1 - 1.5*IQR) & data2$time_diff_col< (Q3 + 1.5*IQR))

#Original df = 7419060 after removing outliers = 7163571
#taking subset of data2
data2_subset <- subset(new_data2,format.Date(new_data2[['StartTime.Date']], "%m")=="12" &
format.Date(new_data2[['StartTime.Date']], "%y")=="21" &
Type %in% c('Rain','Snow') &
Severity %in% c('Light','Heavy') &
City %in% c('New Cumberland','New York','North Clarendon','North Dansville', 'North Kingstown','North Las Vegas','North Little Rock'))
View(data2_subset)

#accessing required data from data2
new_data2 <- data2_subset[c("StartTime.UTC.","Severity","Type")]

#Dropping null values
new_data2 <- drop_na(new_data2)

#assigning sequence to rownames after removing the unnecessary rows rownames(new_data2) <- 1:nrow(new_data2)
View(new_data2)

#to export the files into csv format
write.csv(new_data2,file="data2.csv")

#Reading the csv file
data2 <- read.csv(file="data2.csv", header=TRUE,sep=",")

#Getting the Inter arrival Time
data2 <- data2 %>% mutate(time_diff_col = strptime(data2$StartTime.UTC., "%Y- %m-%d %H:%M:%S") - lag(strptime(data2$StartTime.UTC., "%Y-%m-%d %H:%M:%S")))
View(data2)
#converting Inter arrival time to numeric to do the data analysis data2$time_diff_col <- as.numeric(data2$time_diff_col) View(data2)

#Removing null value
data2 <- data2 %>% drop_na()

#Plot the chart.
boxplot(data2$time_diff_col)

#data without the outliers new_data2_wo = 221, therefore 22 outliers are there in this dataset
Q1 <- quantile(data2$time_diff_col, .25)
Q3 <- quantile(data2$time_diff_col, .75)
IQR <- IQR(data2$time_diff_col)
new_data2_wo <- subset(data2, data2$time_diff_col> (Q1 - 1.5*IQR) & data2$time_diff_col<
(Q3 + 1.5*IQR))
new_data2_wo
#new_data2_wo has 213 data, therefore outliers = 22

#Summary
full_sum_data2 <- summary(data2)
full_sum_data2
sum_data2_severity <- tapply(data2$time_diff_col, data2$Severity, summary) sum_data2_severity
sum_data2_type <- tapply(data2$time_diff_col, data2$Type, summary) sum_data2_type

#SD
sddata1 <- sd(data2$time_diff_col)
sddata1
sd_sev <- aggregate(x= data2$time_diff_col,by = list(data2$Severity), FUN = sd)
sd_sev
sd_type <- aggregate(x= na.omit(data2$time_diff_col),by = list(data2$Type), FUN = sd)
sd_type

#Variance
var_overall = var(data2$time_diff_col)
var_overall
var_sev <- aggregate(x= data2$time_diff_col,by = list(data2$Severity), FUN = var)
var_sev var_typ <- aggregate(x= data2$time_diff_col,by = list(data2$Type), FUN = var)
var_typ
#Other Descriptive Statistics describe(data2$time_diff_col) describeBy(data2$time_diff_col,data2$Severity) describeBy(data2$time_diff_col,data2$Type)

#Exponential Mean
expmean_d2 <- exp(mean(log(data2$time_diff_col)))
expmean_d2
#exponential mean for day_of_the_week by creating a function exp_d2_sev <- function() {
     uniqval <- unique(data2$Severity)
     h <- hash()
     for (i in uniqval){
           d <- data2$time_diff_col[data2$Severity==i]
           a <- exp(mean(log(d)))
h[i]<-a
} return(h)
}
exp_d2_se()
exp_d2_type<- function() {
     uniqval <- unique(dat 2$Type)
     h <- hash()
     for (i in uniqval){
           d <- data2$time_diff_col[data2$Type==i]
           a <- exp(mean(l og(d)))
           h[i]<-a
           }
return(h) }
exp_d2_type()

#Geometric Mean
geomeand2 <- geometric. mean(data2$time_diff_col) geomeand2
geo_sev <- function() {
 uniqval <- unique(data2$Severity)
 h <- hash()
     for (i in uniqval){
           d <- data2$time_diff_col[data2$Severity==i]
           a <- geometric.mean(d)
           h[i]<-a
}
return(h) }
geo_sev()
geo_ty <- function() {
 uniqval <- unique(data2$Type)
 h <- hash()
     for (i in uniqval){
           d <- data2$time_diff_col[data2$Type==i]
           a <- geometric.mean(d)
           h[i]<-a
           }
return(h)
} geo_ty()

#Mode
getmoded2 <- function(v) {
uniqv <- unique(v)uniqv[which.max(tabulate(match(v, uniqv)))] }
mod2 <- getmoded2(data2$time_diff_col)
mod2
mod_d2_sev <- function() {
 uniqval <- unique(data2$Severity)
 h <- hash()
 for (i in uniqval){
d <- data2$time_diff_col[data2$Severity==i] uniqv <- unique(d)
a <- uniqv[which.max(tabulate(match(d, uniqv)))] h[i]<-a
}
return(h) }
mod_d2_sev()
mod_d2_type <- function() {
 uniqval <- unique(data2$Type)
 h <- hash()
for (i in uniqval){
d <- data2$time_diff_col[data2$Type==i]
uniqv <- unique(d)
a <- uniqv[which.max(tabulate(match(d, uniqv)))] h[i]<-a
} return(h) }
mod_d2_type()

#histogram
hist(data2$timediff_col,xlab = "Inter Arrival Time in seconds",col = "lightblue",border = "black", breaks = 10,main='Histogram of Inter Arrival Time')

#### Goodness of Fit ####
# For Dataset1
data1 <- read.csv(file="data1.csv", header=TRUE,sep=",")
a <- min(data1$cost_of_the_order)
b <- max(data1$cost_of_the_order)
intervals<-seq(4,34,length.out = 7)
h <- hist(data1$cost_of_the_order,xlab = "Cost of Order",col = "lightblue",border = "black", xlim = c(0,40),breaks=6,main='Histogram of Cost of Order')
#h <- hist(data1$cost_of_the_order, xlab = "Cost of Order", col = "lightblue", border = "black", xlim = c(0, 40), breaks = 6, main = "Histogram of Cost of Order")
text(h$mids, h$counts, labels = h$counts)
cout <- h$counts
class_probabilities <- list()
class_probabilities <- append(class_probabilities,pnorm(4,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,pnorm(9,mean = 16.65, sd= 7.3)-pnorm(4,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,pnorm(14,mean = 16.65, sd= 7.3)-pnorm(9,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,pnorm(19,mean = 16.65, sd= 7.3)-pnorm(14,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,pnorm(24,mean = 16.65, sd=7.3)-pnorm(19,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,pnorm(29,mean = 16.65, sd= 7.3)-pnorm(24,mean = 16.65, sd= 7.3))
class_probabilities <- append(class_probabilities,1 -pnorm(29,mean = 16.65, sd= 7.3)) sum(unlist(class_probabilities))
exp_val <- lapply(class_probabilities, function(n) n * 551) chi_fun <- function(cout,exp_val){
 blist <- list()
 c <- unlist(cout)
 e <- unlist(exp_val)
 for ( i in 1:(length(cout))){
     oi <- as.numeric(c[i])
     ei <- as.numeric(e[i])
     x <- ((oi-ei)^2)/ei
     blist <- append(blist,x)
 }
return (blist)
}
chi_fun_list <- chi_fun(cout,exp_val)

#For chi-square with alpha = 0.05
v <- qchisq(.95, 6)

#For dataset2
data2 <- read.csv(file="data2.csv", header=TRUE,sep=",") #Getting the Inter arrival Time
data2 <- data2 %>% mutate(time_diff_col = strptime(data2$StartTime.UTC., "%Y- %m-%d %H:%M:%S") - lag(strptime(data2$StartTime.UTC., "%Y-%m-%d %H:%M:%S"))) View(data2)
#converting Inter arrival time to numeric to do the data analysis data2$time_diff_col <- as.numeric(data2$time_diff_col) View(data2) #Removing null value
fdata <- data2 %>% drop_na()
histo<- hist(samples,from=0,to=260000 ,length.out=12,xlab="Inter Arrival Time (secs)",col="lightblue",border="black"),
text(histo$mids, histo$counts, labels = histo$counts)
intervals<- histo$breaks
#observed values
frequency<- histo$counts
class_probabilities <- list()
class_probabilities <- append(class_probabilities,pexp(20000,rate = 1/11334)) class_probabilities <- append(class_probabilities,pexp(40000,rate = 1/11334)- pexp(20000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(60000,rate = 1/11334)- pexp(40000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(80000,rate = 1/11334)- pexp(60000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(100000,rate = 1/11334)- pexp(80000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(120000,rate = 1/11334)-pexp(100000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(140000,rate = 1/11334)- pexp(120000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(160000,rate = 1/11334)- pexp(140000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(180000,rate = 1/11334)- pexp(160000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(200000,rate = 1/11334)- pexp(180000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(220000,rate = 1/11334)- pexp(200000,rate = 1/11334))
class_probabilities <- append(class_probabilities,pexp(240000,rate = 1/11334)- pexp(220000,rate = 1/11334))
class_probabilities <- append(class_probabilities,1 -pexp(240000,rate = 1/11334))
exp_val <- lapply(class_probabilities, function(n) n * 235)
chi_fun <- function(frequency,exp_val){
 blist <- list()
 c <- unlist(frequency)
 e <- unlist(exp_val)
 for ( i in 1:(length(frequency))){
     oi <- as.numeric(c[i])
     ei <- as.numeric(e[i])
     x <- ((oi-ei)^2)/ei
     blist <- append(blist,x)
     }
 return (blist)
}
chi_fun_list <- chi_fun(frequency,exp_val)

#For chi-square with alpha = 0.05
v <- qchisq(.95, 4)

### Hypothesis Testing###

result <- t.test(data1$cost_of_the_order~day_of_the_week,data = data1) p_value <- result$p.value
l <-length(data2$time_diff_col)-1
v <- pchisq(0.05, 6, lower.tail = FALSE)
full_model <- lm(data1$cost_of_the_order~rating,data = data1, subset = rating %in% c("4","5"))
reduced_model <- lm(data1$cost_of_the_order~1,data = data1, subset = rating %in% c("4","5") )
result2 <- anova(reduced_model,full_model)
result2
p_value2 <- result2["Pr(>F)"]
p_value2

